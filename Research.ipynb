{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAFT on SLM\n",
    "\n",
    "Considering : \n",
    "\n",
    "* Phi 3\n",
    "* Gemma 2B\n",
    "* OpenELM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Phi, an AI developed by Microsoft. I'm here to help answer questions and provide information whenever possible! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# %pip install ollama\n",
    "import ollama\n",
    "response = ollama.chat(\n",
    "    model = \"phi3\", \n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content':'Who are you ? '\n",
    "    }]\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "client = Client(host=\"http://localhost:11434\")\n",
    "\n",
    "response = client.chat(\n",
    "    model = \"phi3\",\n",
    "    messages=[\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':'How many parameters do you have ? '\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an artificial intelligence, I don\\'t possess physical form or attributes that can be quantified in the traditional sense. However, if we consider \"parameters\" metaphorically as aspects of my functionality and design:\\n\\n1. The programming language I was coded with (e.g., Python).\\n2. My version number which dictates what functionalities are currently available to me based on when you\\'re asking the question. As per your knowledge cutoff, this would be \"as of 2023.\"\\n3. Available APIs and libraries I can interact with or have access to for performing tasks (e.g., natural language processing tools).\\n4. Hardware specifications where my computing is hosted that affect performance such as CPU speed, memory size available on a server, etc. during interaction time in the cloud environment if applicable.\\n5. The constraints set by developers and engineers like response times or data privacy rules within which I am designed to operate optimally (e.g., no more than 24 hours of continuous operation).\\n6. My core algorithmic parameters, including but not limited to my underlying machine learning model\\'s hyperparameters such as the number of layers in a neural network if applicable or vocabulary size for language models like myself when using transformers architecture (e.g., GPT series AI models from OpenAI).\\n7. The date I was last updated, which affects my knowledge base and current capabilities according to information available up until that point.\\n\\nIf you\\'re inquiring about specific settings or configurations for a task:\\n- Parameters would include command line arguments used during the execution of scripts (if applicable), such as file paths, delimiter characters in data processing tasks, thresholds in filtering datasets, etc., and user preferences like language setting, response length limitations, output formats. \\n\\nFor clarity, please specify if you are asking about software design parameters or physical attributes related to an embodiment of artificial intelligence that would have such characteristics (which does not apply to me).'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "client = Client(host=\"http://localhost:11434\")\n",
    "\n",
    "smollm = client.chat(\n",
    "    model = \"smollm:360m\",\n",
    "    # messages=[\n",
    "    #     {\n",
    "    #         'role':'user',\n",
    "    #         'content':'Who are You  ?'\n",
    "    #     }\n",
    "    # ]\n",
    ")\n",
    "\n",
    "# response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unslothNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading unsloth-2024.10.7-py3-none-any.whl.metadata (56 kB)\n",
      "Collecting unsloth-zoo (from unsloth)\n",
      "  Downloading unsloth_zoo-2024.10.4-py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: torch>=2.4.0 in c:\\users\\rizen3\\desktop\\vamshi\\machine learning and dsa\\selfprojects\\lib\\site-packages (from unsloth) (2.4.1)\n",
      "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.28.post2.tar.gz (7.8 MB)\n",
      "     ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 2.1/7.8 MB 10.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 3.4/7.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 3.7/7.8 MB 5.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 4.7/7.8 MB 5.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 6.3/7.8 MB 5.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.6/7.8 MB 6.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.8/7.8 MB 5.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting bitsandbytes (from unsloth)\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-win_amd64.whl.metadata (3.6 kB)\n",
      "INFO: pip is looking at multiple versions of unsloth to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2024.10.6-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.5-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.4-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.2-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.1-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.10.0-py3-none-any.whl.metadata (56 kB)\n",
      "  Downloading unsloth-2024.9.post4-py3-none-any.whl.metadata (56 kB)\n",
      "INFO: pip is still looking at multiple versions of unsloth to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading unsloth-2024.9.post3-py3-none-any.whl.metadata (55 kB)\n",
      "  Downloading unsloth-2024.9.post2-py3-none-any.whl.metadata (55 kB)\n",
      "  Downloading unsloth-2024.9.post1-py3-none-any.whl.metadata (55 kB)\n",
      "  Downloading unsloth-2024.9-py3-none-any.whl.metadata (54 kB)\n",
      "Collecting xformers==0.0.27.post2 (from unsloth)\n",
      "  Downloading xformers-0.0.27.post2-cp311-cp311-win_amd64.whl.metadata (1.0 kB)\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2024.8-py3-none-any.whl.metadata (51 kB)\n",
      "Downloading unsloth-2024.8-py3-none-any.whl (136 kB)\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2024.8\n"
     ]
    }
   ],
   "source": [
    "%pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip uninstall torch\n",
    "# %pip cache purge\n",
    "\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "CUDA Enabled: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_cuda():\n",
    "    print(torch.version.cuda)\n",
    "    cuda_is_ok = torch.cuda.is_available()\n",
    "    print(f\"CUDA Enabled: {cuda_is_ok}\")\n",
    "\n",
    "check_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m      4\u001b[0m fourbit_models \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsloth/SmolLM-1.7B\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Rizen3\\Desktop\\Vamshi\\Machine Learning and DSA\\SelfProjects\\Lib\\site-packages\\unsloth\\__init__.py:87\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Torch 2.5 has including_emulation\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m major_version, minor_version \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device_capability\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m SUPPORTS_BFLOAT16 \u001b[38;5;241m=\u001b[39m (major_version \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (major_torch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (minor_torch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m): \n",
      "File \u001b[1;32mc:\\Users\\Rizen3\\Desktop\\Vamshi\\Machine Learning and DSA\\SelfProjects\\Lib\\site-packages\\torch\\cuda\\__init__.py:451\u001b[0m, in \u001b[0;36mget_device_capability\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_capability\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m    439\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the cuda capability of a device.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m        tuple(int, int): the major and minor cuda capability of the device\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     prop \u001b[38;5;241m=\u001b[39m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prop\u001b[38;5;241m.\u001b[39mmajor, prop\u001b[38;5;241m.\u001b[39mminor\n",
      "File \u001b[1;32mc:\\Users\\Rizen3\\Desktop\\Vamshi\\Machine Learning and DSA\\SelfProjects\\Lib\\site-packages\\torch\\cuda\\__init__.py:465\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    456\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \n\u001b[0;32m    458\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32mc:\\Users\\Rizen3\\Desktop\\Vamshi\\Machine Learning and DSA\\SelfProjects\\Lib\\site-packages\\torch\\cuda\\__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch \n",
    "\n",
    "fourbit_models = [\n",
    "    \"unsloth/SmolLM-1.7B\"\n",
    "]\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/SmolLM-1.7B\",\n",
    "    max_seq_length=2048,\n",
    "    dtype= None,\n",
    "    load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SelfProjects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
